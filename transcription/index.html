<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Orbitz Transcription UI - Web Speech API</title>
    <style>
        :root {
            --font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            
            /* Orbitz Branding Colors */
            --orbitz-dark-blue: #002e5d; 
            --orbitz-primary-blue: #0077c8;
            --orbitz-accent: #00a69c;

            /* Light Theme Variables */
            --bg-primary: #F7F9FC;
            --bg-secondary: #FFFFFF;
            --text-primary: #222F3E;
            --text-secondary: #576574;
            --accent-primary: var(--orbitz-primary-blue);
            --accent-secondary: var(--orbitz-dark-blue);
            --border-color: #DDE3EA;
            --shadow-color: rgba(0, 46, 93, 0.08);
            --error-color: #EF4444;
            --success-color: #10B981;
            --output-bg: #FDFEFE;

            /* Universal */
            --border-radius: 8px;
            --transition-speed: 0.25s;
            --button-padding: 0.75em 1.4em;
        }

        /* Global Styles */
        *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
        html { scroll-behavior: smooth; }
        body {
            font-family: var(--font-family); 
            background-color: var(--bg-primary); 
            color: var(--text-primary);
            transition: background-color var(--transition-speed) ease, color var(--transition-speed) ease;
            font-size: 16px; 
            line-height: 1.6;
        }
        
        /* UPDATED: Widescreen Container */
        .container { 
            width: 100%; 
            max-width: 98%; /* Changed from 1200px to 98% for widescreen */
            margin: 0 auto; 
            padding: 1rem 2rem; 
        }

        /* Header */
        .app-header {
            display: flex; 
            justify-content: space-between; 
            align-items: center; 
            padding: 0.75rem 2rem; /* Matches container padding */
            background-color: var(--bg-secondary); 
            box-shadow: 0 2px 8px var(--shadow-color);
            border-bottom: 1px solid var(--border-color); 
            position: sticky; 
            top: 0; 
            z-index: 1000;
        }
        
        .logo-text { 
            font-size: 1.5rem;
            font-weight: 800;
            color: var(--orbitz-dark-blue);
            letter-spacing: -0.5px;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .logo-text span {
            color: var(--orbitz-accent);
        }
        
        .main-content { 
            padding-top: 1.5rem; 
            display: flex; 
            flex-direction: column; 
            gap: 1.5rem; 
        }
        
        .card {
            background-color: var(--bg-secondary); 
            border-radius: var(--border-radius); 
            padding: 1.5rem;
            box-shadow: 0 4px 12px var(--shadow-color); 
            border: 1px solid var(--border-color);
            transition: all var(--transition-speed) ease;
        }
        
        .card-title { 
            font-size: 1.1rem; 
            font-weight: 600; 
            margin-bottom: 1rem; 
            color: var(--text-primary); 
        }
        
        .controls-area .control-group { 
            margin-bottom: 1rem; 
        }
        
        .controls-area label { 
            display: block; 
            font-size: 0.875rem; 
            color: var(--text-secondary); 
            margin-bottom: 0.4rem; 
        }
        
        .controls-area select, 
        .controls-area input[type="url"], 
        .status-display-text { 
            width: 100%; 
            padding: 0.65rem 0.75rem; 
            border-radius: var(--border-radius); 
            border: 1px solid var(--border-color);
            background-color: var(--bg-primary); 
            color: var(--text-primary); 
            font-size: 0.9rem;
            transition: all var(--transition-speed) ease;
        }
        
        .controls-area select:focus, 
        .controls-area input[type="url"]:focus { 
            outline: none; 
            border-color: var(--accent-primary); 
            box-shadow: 0 0 0 2px rgba(0, 119, 200, 0.2);
        }

        .status-display-container { 
            opacity: 1; 
            transform: translateY(0); 
            transition: all var(--transition-speed) ease; 
        }
        
        .status-display-text {
            min-height: 38px; 
            display: flex; 
            align-items: center; 
            font-style: italic; 
            margin-bottom: 0.5rem;
        }
        
        .status-display-text.error { 
            color: var(--error-color); 
            font-weight: 500; 
            border-left: 3px solid var(--error-color); 
            padding-left: 0.5rem;
            font-style: normal;
        }
        
        .status-display-text.success { 
            color: var(--success-color); 
            font-weight: 500; 
            border-left: 3px solid var(--success-color); 
            padding-left: 0.5rem;
            font-style: normal;
        }
        
        #audioWaveformCanvas {
            width: 100%; 
            height: 60px; 
            background-color: var(--output-bg);
            border-radius: calc(var(--border-radius) / 2); 
            display: none;
            border: 1px solid var(--border-color);
            margin-top: 1rem;
        }
        
        .button-group { 
            display: flex; 
            flex-direction: column; 
            gap: 0.75rem; 
        }
        
        .button-row { 
            display: flex; 
            gap: 0.5rem; 
        }
        
        .button-row .btn { 
            flex: 1; 
        }

        .btn {
            padding: var(--button-padding); 
            font-size: 0.95rem; 
            font-weight: 500; 
            border: 1.5px solid transparent;
            border-radius: var(--border-radius); 
            cursor: pointer;
            transition: all var(--transition-speed) ease;
            text-align: center; 
            width: 100%;
            box-shadow: 0 2px 4px var(--shadow-color);
        }
        
        .btn-primary { 
            background-color: var(--accent-primary); 
            color: white; 
            border-color: var(--accent-primary);
        }
        
        .btn-primary:hover:not(:disabled) { 
            background-color: var(--accent-secondary); 
            border-color: var(--accent-secondary);
            box-shadow: 0 4px 8px var(--shadow-color); 
        }
        
        .btn-secondary { 
            background-color: var(--bg-secondary); 
            color: var(--accent-primary); 
            border: 1.5px solid var(--accent-primary); 
        }
        
        .btn-secondary:hover:not(:disabled) { 
            background-color: rgba(0, 119, 200, 0.1);
            box-shadow: 0 4px 8px var(--shadow-color); 
        }
        
        .btn-warning { 
            background-color: #F59E0B; 
            color: white; 
            border-color: #F59E0B;
        }
        
        .btn-warning:hover:not(:disabled) { 
            background-color: #D97706;
            border-color: #D97706;
        }
        
        /* Purple style for System Audio */
        .btn-purple { 
            background-color: #8B5CF6; 
            color: white; 
            border-color: #8B5CF6;
        }
        .btn-purple:hover:not(:disabled) { 
            background-color: #7C3AED;
            border-color: #7C3AED;
            box-shadow: 0 4px 8px var(--shadow-color); 
        }

        .btn:active:not(:disabled) { 
            transform: scale(0.98); 
            box-shadow: 0 1px 2px var(--shadow-color); 
        }
        
        .btn.disabled, 
        .btn:disabled {
            background-color: #9ca3af !important; 
            color: #e5e7eb !important;
            cursor: not-allowed !important; 
            opacity: 0.7 !important; 
            border-color: #9ca3af !important;
            box-shadow: none !important;
        }
        
        .upload-group { 
            margin-top: 0.75rem; 
        }
        
        #audioFileUpload { 
            display: none; 
        }
        
        .btn-upload { 
            background-color: var(--orbitz-accent); 
            color: white;
            border-color: var(--orbitz-accent);
        }
        
        .btn-upload:hover:not(:disabled) { 
            background-color: #008f86; 
            border-color: #008f86;
            box-shadow: 0 4px 8px var(--shadow-color);
        }
        
        #fileNameDisplay { 
            font-size: 0.8rem; 
            color: var(--text-secondary); 
            margin-top: 0.4rem; 
            text-align: center; 
            word-break: break-all; 
            min-height: 1.2rem;
        }
        
        .url-group { 
            margin-top: 1.5rem; 
            padding-top: 1rem; 
            border-top: 1px solid var(--border-color); 
        }
        
        .url-group input[type="url"] { 
            margin-bottom: 0.75rem; 
        }

        .output-column { 
            display: flex; 
            flex-direction: column; 
            gap: 1.5rem; 
        }
        
        .output-area {
            width: 100%; 
            min-height: 120px; 
            padding: 0.75rem 1rem; 
            border-radius: var(--border-radius);
            border: 1px solid var(--border-color); 
            background-color: var(--output-bg); 
            color: var(--text-primary);
            font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace;
            font-size: 0.875rem; 
            line-height: 1.6; 
            resize: vertical;
            transition: all var(--transition-speed) ease;
            white-space: pre-wrap; 
            word-wrap: break-word;
        }

        .output-area .interim-transcript {
            color: var(--text-secondary);
            opacity: 0.8;
        }
        
        .output-area:focus { 
            outline: none; 
            border-color: var(--accent-primary); 
            box-shadow: 0 0 0 2px rgba(0, 119, 200, 0.2); 
        }
        
        #detectedTopicsList { 
            list-style-type: none; 
            padding-left: 0; 
        }
        
        #detectedTopicsList li {
            background-color: var(--bg-primary); 
            padding: 0.5rem 0.75rem; 
            border-radius: calc(var(--border-radius) / 2);
            margin-bottom: 0.5rem; 
            border: 1px solid var(--border-color); 
            font-size: 0.9rem;
            color: var(--text-primary);
        }
        
        #detectedTopicsList li:last-child { 
            margin-bottom: 0; 
        }
        
        #detectedTopicsList.empty { 
            font-style: italic; 
            color: var(--text-secondary); 
        }
        
        .placeholder-note { 
            font-size: 0.8rem; 
            color: var(--text-secondary); 
            margin-top: 0.5rem; 
            font-style: italic;
        }

        @media (min-width: 768px) {
            .container { 
                padding: 1.5rem 2rem; 
            }
            
            .main-content { 
                flex-direction: row; 
                align-items: flex-start; 
            }
            
            .controls-column { 
                flex: 0 0 320px; 
                max-width: 320px; 
                display: flex; 
                flex-direction: column; 
                gap: 1.5rem; 
            }
            
            .transcript-column-wrapper { 
                flex: 1; 
                min-width: 0; 
            }
        }
        
        .app-footer {
            text-align: center; 
            padding: 1.5rem 1rem; 
            color: var(--text-secondary); 
            font-size: 0.875rem;
            border-top: 1px solid var(--border-color); 
            margin-top: 2rem;
        }
        
        #loadingOverlay {
            position: fixed; 
            top: 0; 
            left: 0; 
            width: 100%; 
            height: 100%;
            background-color: rgba(255, 255, 255, 0.8);
            -webkit-backdrop-filter: blur(4px);
            backdrop-filter: blur(4px);
            display: flex; 
            flex-direction: column; 
            justify-content: center; 
            align-items: center;
            z-index: 2000; 
            color: var(--text-primary); 
            font-size: 1.2rem; 
            text-align: center;
            opacity: 0; 
            visibility: hidden; 
            transition: all 0.3s ease;
        }
        
        #loadingOverlay.visible { 
            opacity: 1; 
            visibility: visible; 
        }
        
        .spinner {
            border: 5px solid #e5e7eb; 
            border-top: 5px solid var(--orbitz-primary-blue);
            border-radius: 50%; 
            width: 50px; 
            height: 50px;
            animation: spin 1s linear infinite; 
            margin-bottom: 1rem;
        }
        
        @keyframes spin { 
            0% { transform: rotate(0deg); } 
            100% { transform: rotate(360deg); } 
        }
        
        .recorded-audio-list-section { 
            margin-top: 1.5rem; 
        }
        
        #recordedAudioList { 
            list-style-type: none; 
            padding-left: 0; 
        }
        
        #recordedAudioList li {
            background-color: var(--bg-secondary);
            border: 1px solid var(--border-color);
            border-radius: var(--border-radius);
            padding: 1rem;
            margin-bottom: 1rem;
            box-shadow: 0 2px 4px var(--shadow-color);
        }
        
        #recordedAudioList li:last-child { 
            margin-bottom: 0; 
        }
        
        #recordedAudioList .empty-list-message {
            text-align: center;
            color: var(--text-secondary);
            padding: 1rem;
            background-color: transparent;
            border: none;
            box-shadow: none;
        }
        
        #recordedAudioList audio { 
            width: 100%; 
            margin-top: 0.5rem; 
        }
        
        #recordedAudioList .audio-metadata { 
            font-size: 0.8rem; 
            color: var(--text-secondary); 
            margin-top: 0.25rem; 
        }
        
        #recordedAudioList .transcript-history {
            background-color: var(--bg-primary);
            border: 1px solid var(--border-color);
            border-radius: 4px;
            padding: 0.75rem;
            margin-top: 0.75rem;
            font-size: 0.85rem;
            white-space: pre-wrap;
            word-wrap: break-word;
            max-height: 150px;
            overflow-y: auto;
            color: var(--text-secondary);
        }
    </style>
</head>
<body>
    <div id="loadingOverlay">
        <div class="spinner"></div>
        <p id="loadingMessage">Processing audio...</p>
    </div>

    <header class="app-header">
        <div class="logo-text">Orbitz<span>.ai</span></div>
    </header>

    <div class="container">
        <main class="main-content">
            <div class="controls-column">
                <section class="card controls-area">
                    <h2 class="card-title">Transcription Controls</h2>
                    
                    <div class="button-group">
                        <button id="startRecordBtn" class="btn btn-primary">Start Mic Recording (Batch)</button>
                        <button id="startLiveBtn" class="btn btn-secondary">Start Live Mic Transcription (Web Speech)</button>
                        <button id="startSystemAudioBtn" class="btn btn-purple">Record System/Tab Audio</button>
                        
                        <div class="button-row">
                            <button id="pauseRecordBtn" class="btn btn-warning disabled" disabled>Pause</button>
                            <button id="stopRecordBtn" class="btn btn-secondary disabled" disabled>Stop</button>
                        </div>
                        <div class="upload-group">
                            <button id="uploadAudioBtn" class="btn btn-upload">Upload Audio File</button>
                            <input type="file" id="audioFileUpload" accept="audio/*,video/webm,audio/webm,audio/mp3,audio/mp4,audio/mpeg,audio/mpga,audio/m4a,audio/wav,audio/ogg">
                            <div id="fileNameDisplay">No file selected.</div>
                        </div>
                        <div class="url-group">
                            <input type="url" id="audioUrlInput" placeholder="Enter Audio URL (.webm, .mp3, etc.)">
                            <button id="transcribeUrlBtn" class="btn btn-primary">Transcribe from URL</button>
                        </div>
                        <canvas id="audioWaveformCanvas"></canvas> 
                    </div>
                </section>

                <section class="card status-area">
                    <h2 class="card-title">Status</h2>
                    <div class="status-display-container">
                        <div id="statusDisplayText" class="status-display-text">Idle. Ready to record or upload.</div>
                    </div>
                </section>
            </div>

            <div class="transcript-column-wrapper output-column">
                <section class="card">
                    <h2 class="card-title">Real-time Transcription (Web Speech API)</h2>
                    <div id="transcriptOutputConvo" class="output-area" contenteditable="false"></div>
                </section>
                <section class="card">
                    <h2 class="card-title">Multi-Speaker Output (from Finalized Transcript)</h2>
                    <p class="placeholder-note">Note: Multi-speaker diarization is not available with standard Web Speech API.</p>
                    <textarea id="transcriptOutputMultispeaker" class="output-area" placeholder="A detailed, timestamped transcript with speaker labels will appear here after processing (Batch mode only)." readonly></textarea>
                </section>

                <!-- NEW: TTS CARD -->
                <section class="card">
                    <h2 class="card-title">Read Transcript Aloud (TTS)</h2>
                    <div class="controls-area">
                        <div class="control-group">
                            <label for="ttsVoiceSelect">TTS Voice (Deepgram Aura)</label>
                            <select id="ttsVoiceSelect">
                                <option value="aura-2-thalia-en">Thalia · en-US · clear, energetic</option>
                                <option value="aura-2-andromeda-en">Andromeda · en-US · expressive</option>
                                <option value="aura-2-helena-en">Helena · en-US · friendly</option>
                                <option value="aura-2-apollo-en">Apollo · en-US · confident</option>
                                <option value="aura-2-arcas-en">Arcas · en-US · smooth</option>
                            </select>
                        </div>
                        <div class="control-group">
                            <label for="audioOutputSelect">Audio Output Device</label>
                            <select id="audioOutputSelect">
                                <option value="">System default output</option>
                            </select>
                            <p class="placeholder-note" id="audioOutputSupportNote">
                                Choose which speaker/headset will play TTS audio. Works best in Chrome/Edge over HTTPS.
                            </p>
                        </div>
                        <div class="button-row">
                            <button id="ttsPlayBtn" class="btn btn-primary">Speak Transcript</button>
                            <button id="ttsStopBtn" class="btn btn-secondary">Stop</button>
                        </div>
                    </div>
                </section>
                <!-- END TTS CARD -->

                <section class="card">
                    <h2 class="card-title">Detected Topics</h2>
                    <p class="placeholder-note">Note: Topic detection is not available with standard Web Speech API.</p>
                    <ul id="detectedTopicsList" class="empty">
                        <li>No topics detected yet.</li>
                    </ul>
                </section>
            </div>
        </main>
        <section class="card recorded-audio-list-section">
            <h2 class="card-title">Recorded Audio History</h2>
            <ul id="recordedAudioList">
                <li class="empty-list-message">No recordings yet.</li>
            </ul>
        </section>
    </div>

    <footer class="app-footer">
        <p>© <span id="currentYear"></span> Orbitz.ai</p>
    </footer>

    <script type="module">
        import { initializeApp } from "https://www.gstatic.com/firebasejs/9.22.0/firebase-app.js";
        import { getStorage, ref, uploadBytes } from "https://www.gstatic.com/firebasejs/9.22.0/firebase-storage.js";

        document.addEventListener('DOMContentLoaded', () => {
            // --- CONFIGURATION (Kept for Batch/System modes) ---
            const DEEPGRAM_API_KEY = "627dcee0be8a2ce68d35f5cf31c2052287951602"; 
            const FIREBASE_CONFIG = {
                apiKey: "AIzaSyCjVuc2VD5YvJE_4PBUJATmKiJzFC1ex8c",
                authDomain: "aitek2023-8f504.firebaseapp.com",
                projectId: "aitek2023-8f504",
                storageBucket: "aitek2023-8f504.appspot.com",
                messagingSenderId: "570516064142",
                appId: "1:570516064142:web:383ef4de00b5f48f5886df"
            };

            // --- FIREBASE & DB SETUP ---
            let app, storage;
            try { app = initializeApp(FIREBASE_CONFIG); storage = getStorage(app); } catch (e) { console.error("Firebase init failed:", e); }
            const DB_NAME = 'OrbitzAudioDB', STORE_NAME = 'audioFiles';
            let localDB;

            // --- DOM ELEMENTS ---
            const loadingOverlay = document.getElementById('loadingOverlay'),
                  loadingMessage = document.getElementById('loadingMessage'),
                  statusDisplayText = document.getElementById('statusDisplayText'),
                  startRecordBtn = document.getElementById('startRecordBtn'),
                  startLiveBtn = document.getElementById('startLiveBtn'),
                  startSystemAudioBtn = document.getElementById('startSystemAudioBtn'),
                  pauseRecordBtn = document.getElementById('pauseRecordBtn'),
                  stopRecordBtn = document.getElementById('stopRecordBtn'),
                  transcriptOutputConvo = document.getElementById('transcriptOutputConvo'),
                  transcriptOutputMultispeaker = document.getElementById('transcriptOutputMultispeaker'),
                  detectedTopicsList = document.getElementById('detectedTopicsList'),
                  uploadAudioBtn = document.getElementById('uploadAudioBtn'),
                  audioFileUpload = document.getElementById('audioFileUpload'),
                  fileNameDisplay = document.getElementById('fileNameDisplay'),
                  audioUrlInput = document.getElementById('audioUrlInput'),
                  transcribeUrlBtn = document.getElementById('transcribeUrlBtn'),
                  waveformCanvas = document.getElementById('audioWaveformCanvas'),
                  waveformCtx = waveformCanvas.getContext('2d'),
                  recordedAudioList = document.getElementById('recordedAudioList'),
                  // NEW: TTS controls
                  ttsVoiceSelect = document.getElementById('ttsVoiceSelect'),
                  ttsPlayBtn = document.getElementById('ttsPlayBtn'),
                  ttsStopBtn = document.getElementById('ttsStopBtn'),
                  audioOutputSelect = document.getElementById('audioOutputSelect'),
                  audioOutputSupportNote = document.getElementById('audioOutputSupportNote');

            // --- APP STATE ---
            // recordingState can be: 'idle', 'batch-recording', 'system-audio', 'paused', 'processing', OR 'webspeech'
            let recordingState = 'idle'; 
            
            // Deepgram/MediaRecorder State (for Batch/System)
            let mediaRecorder, currentStream, audioContext, mediaStreamSource, analyserNode, animationFrameId, deepgramSocket;
            let allRecordedBlobs = [];

            // Web Speech API State
            let recognition;
            let finalWebSpeechTranscript = '';

            // NEW: TTS STATE
            let ttsAudioElement = null;
            let ttsObjectUrl = null;
            const MAX_TTS_CHARS = 1900; // keep under Deepgram 2000-char limit per request

            // --- INITIALIZATION ---
            function main() {
                document.getElementById('currentYear').textContent = new Date().getFullYear();
                setupEventListeners();
                setupTTS();
                checkApiKey(); // Checks Deepgram key for batch mode stability
            }

            // --- UTILITY FUNCTIONS ---
            function checkApiKey() { 
                if (!DEEPGRAM_API_KEY) { 
                    updateStatus("Deepgram API Key is missing (Batch/TTS disabled).", "error"); 
                    return false; 
                } 
                updateStatus("Idle. Ready.", "info"); 
                return true; 
            }

            function showLoading(message) { 
                loadingMessage.textContent = message; 
                loadingOverlay.classList.add('visible'); 
            }
            function hideLoading() { loadingOverlay.classList.remove('visible'); }

            function updateStatus(message, type = "info") { 
                statusDisplayText.textContent = message; 
                statusDisplayText.className = 'status-display-text'; 
                if (type === 'error') statusDisplayText.classList.add('error'); 
                if (type === 'success') statusDisplayText.classList.add('success'); 
            }

            function updateUIForRecordingState() {
                const isRecording = recordingState.includes('recording') || recordingState === 'system-audio';
                const isWebSpeech = recordingState === 'webspeech';
                const isPaused = recordingState === 'paused';
                const isIdle = recordingState === 'idle';
                const isActive = isRecording || isWebSpeech || isPaused;

                // Disable start controls if any recording mode is active
                [startRecordBtn, startLiveBtn, startSystemAudioBtn, uploadAudioBtn, audioFileUpload, audioUrlInput, transcribeUrlBtn].forEach(el => {
                    el.disabled = isActive;
                    el.classList.toggle('disabled', isActive);
                });

                // Manage Stop/Pause buttons based on mode
                if (isWebSpeech) {
                    pauseRecordBtn.disabled = true;
                    pauseRecordBtn.classList.add('disabled');
                    stopRecordBtn.disabled = false;
                    stopRecordBtn.classList.remove('disabled');
                } else {
                    pauseRecordBtn.disabled = !isRecording;
                    stopRecordBtn.disabled = !isRecording && !isPaused;
                    pauseRecordBtn.classList.toggle('disabled', !isRecording);
                    stopRecordBtn.classList.toggle('disabled', !isRecording && !isPaused);
                }
            }
            
            function clearOutputs() { 
                transcriptOutputConvo.innerHTML = ""; 
                transcriptOutputMultispeaker.value = ""; 
                detectedTopicsList.innerHTML = '<li>No topics detected yet.</li>'; 
                detectedTopicsList.classList.add('empty'); 
                fileNameDisplay.textContent = "No file selected."; 
                audioUrlInput.value = ""; 
                finalWebSpeechTranscript = "";
            }

            function resetToIdle(message, type) {
                // Clean up MediaRecorder stuff
                if (currentStream) currentStream.getTracks().forEach(track => track.stop());
                if (mediaRecorder && mediaRecorder.state !== 'inactive') mediaRecorder.stop();
                if (deepgramSocket && deepgramSocket.readyState < 2) deepgramSocket.close();
                
                // Clean up Web Speech stuff
                if (recognition) {
                    recognition.onend = null;
                    recognition.stop();
                    recognition = null;
                }

                recordingState = 'idle';
                mediaRecorder = currentStream = deepgramSocket = null;
                allRecordedBlobs = [];
                stopWaveformVisualization();
                waveformCanvas.style.display = 'none';
                updateUIForRecordingState();
                if (message && type) { updateStatus(message, type); } else { checkApiKey(); }
                hideLoading();
            }

            function setupEventListeners() {
                startRecordBtn.addEventListener('click', () => startRecording('batch-recording'));
                startLiveBtn.addEventListener('click', startWebSpeech); 
                startSystemAudioBtn.addEventListener('click', () => startRecording('system-audio'));
                pauseRecordBtn.addEventListener('click', togglePause);
                stopRecordBtn.addEventListener('click', stopRecording);
                uploadAudioBtn.addEventListener('click', () => audioFileUpload.click());
                // audioFileUpload.addEventListener('change', handleFileUpload); // Disabled for demo
                // transcribeUrlBtn.addEventListener('click', handleUrlImport); // Disabled for demo
            }

            // --- WEB SPEECH API IMPLEMENTATION ---
            function startWebSpeech() {
                if (recordingState !== 'idle') return;

                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                if (!SpeechRecognition) {
                    updateStatus("Sorry, your browser does not support the Web Speech API.", "error");
                    return;
                }

                clearOutputs();
                updateStatus("Initializing Web Speech API...", "info");
                
                try {
                    recognition = new SpeechRecognition();
                    recognition.continuous = true;
                    recognition.interimResults = true;
                    recognition.lang = 'en-US';

                    recognition.onstart = function() {
                        recordingState = 'webspeech';
                        updateStatus("Listening (Web Speech API)... Speak now.", "success");
                        updateUIForRecordingState();
                        waveformCanvas.style.display = 'none'; 
                    };

                    recognition.onerror = function(event) {
                        let msg = `Web Speech Error: ${event.error}`;
                        if (event.error === 'no-speech') { msg = "No speech was detected. Try again."; }
                        if (event.error === 'audio-capture') { msg = "No microphone was found. Ensure it is plugged in."; }
                        if (event.error === 'not-allowed') { msg = "Microphone permission denied."; }
                        resetToIdle(msg, "error");
                    };

                    recognition.onend = function() {
                        if (recordingState === 'webspeech') {
                             console.log("Web Speech ended unexpectedly, restarting for continuous listening...");
                             try { recognition.start(); } catch(e) { console.log("Restart failed", e); resetToIdle("Transcription stopped.", "info");}
                        } else {
                             resetToIdle("Live transcription stopped.", "info");
                        }
                    };

                    recognition.onresult = function(event) {
                        let interimTranscript = '';
                        let currentFinal = finalWebSpeechTranscript;

                        for (let i = event.resultIndex; i < event.results.length; ++i) {
                            const transcriptSegment = event.results[i][0].transcript;
                            if (event.results[i].isFinal) {
                                finalWebSpeechTranscript += transcriptSegment + ' ';
                                currentFinal = finalWebSpeechTranscript;
                            } else {
                                interimTranscript += transcriptSegment;
                            }
                        }
                        
                        transcriptOutputConvo.innerHTML = currentFinal + `<span class="interim-transcript">${interimTranscript}</span>`;
                        transcriptOutputConvo.scrollTop = transcriptOutputConvo.scrollHeight;
                    };

                    recognition.start();

                } catch (e) {
                    resetToIdle(`Failed to start Web Speech API: ${e.message}`, "error");
                }
            }

            // --- DEEPGRAM / MEDIARECORDER IMPLEMENTATION (For Batch & System) ---
            async function startRecording(mode) {
                if (!DEEPGRAM_API_KEY || recordingState !== 'idle') return; 
                clearOutputs();
                
                try {
                    let stream;
                    if (mode === 'system-audio') {
                        updateStatus("Select tab/screen & share audio...", "info");
                        stream = await navigator.mediaDevices.getDisplayMedia({ video: true, audio: true });
                        const audioTracks = stream.getAudioTracks();
                        if (audioTracks.length === 0) {
                            stream.getTracks().forEach(t => t.stop());
                            resetToIdle("Error: You must select 'Share Audio' or 'Share Tab Audio' in the browser popup.", "error");
                            return;
                        }
                        const audioTrack = audioTracks[0];
                        audioTrack.onended = () => stopRecording(); 
                        currentStream = new MediaStream([audioTrack]);
                        stream.getVideoTracks().forEach(track => track.stop());
                        updateStatus("Capturing System/Tab Audio...", "success");
                    } else {
                        updateStatus("Requesting microphone for batch recording...", "info");
                        stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                        currentStream = stream;
                        updateStatus("Recording for batch processing...", "success");
                    }

                    recordingState = mode;
                    startWaveformVisualization(currentStream);
                    updateUIForRecordingState();
                    
                    const options = { mimeType: 'audio/webm;codecs=opus' };
                    mediaRecorder = new MediaRecorder(currentStream, options);
                    mediaRecorder.ondataavailable = e => { 
                        if (e.data.size > 0) allRecordedBlobs.push(e.data); 
                    };
                    mediaRecorder.onstop = handleRecordingStop;

                    mediaRecorder.start(1000);

                } catch (err) { 
                    resetToIdle(`Audio device error: ${err.message}`, "error"); 
                }
            }
            
            function togglePause() {
                if (!mediaRecorder || recordingState === 'webspeech') return;
                if (recordingState.includes('recording') || recordingState === 'system-audio') {
                    mediaRecorder.pause();
                    mediaRecorder.oldState = recordingState;
                    recordingState = 'paused';
                    updateStatus("Recording paused.", "info");
                } else if (recordingState === 'paused') {
                    mediaRecorder.resume();
                    recordingState = mediaRecorder.oldState || 'batch-recording'; 
                    updateStatus("Recording resumed.", "success");
                }
                updateUIForRecordingState();
            }

            function stopRecording() {
                if (recordingState === 'webspeech' && recognition) {
                    updateStatus("Stopping Web Speech...", "info");
                    recordingState = 'processing'; 
                    recognition.stop();
                    return;
                }

                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    recordingState = 'processing';
                    updateStatus("Finalizing recording...", "info");
                    showLoading("Processing audio for batch transcription...");
                    mediaRecorder.stop(); 
                }
            }
            
            async function handleRecordingStop() {
                if (deepgramSocket && deepgramSocket.readyState < 2) deepgramSocket.close();
                const finalBlob = new Blob(allRecordedBlobs, { type: 'audio/webm' });
                if (finalBlob.size < 2000) { resetToIdle("No substantial audio was recorded.", "info"); return; }
                
                // SIMULATED BATCH TRANSCRIPTION
                setTimeout(() => {
                    const simulatedTranscript = "This is a simulated transcript for the batch recording. In a real implementation, the audio blob would be sent to a transcription API, and the result would appear here.";
                    transcriptOutputConvo.innerHTML = simulatedTranscript;
                    transcriptOutputMultispeaker.value = "[00:00.000] Speaker 0: " + simulatedTranscript;
                    detectedTopicsList.innerHTML = "<li>Simulated Topic A</li><li>Simulated Topic B</li>";
                    detectedTopicsList.classList.remove('empty');
                    resetToIdle("Batch transcription complete (Simulated).", "success");
                }, 2000);
            }

            // --- WAVEFORM VISUALIZATION (Only for MediaRecorder modes) ---
            function startWaveformVisualization(stream) {
                if (!stream.active) return;
                waveformCanvas.style.display = 'block';
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                mediaStreamSource = audioContext.createMediaStreamSource(stream);
                analyserNode = audioContext.createAnalyser();
                analyserNode.fftSize = 256;
                mediaStreamSource.connect(analyserNode);
                drawWaveform();
            }
            
            function drawWaveform() {
                if (recordingState === 'idle' || recordingState === 'webspeech') return;
                const bufferLength = analyserNode.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                analyserNode.getByteFrequencyData(dataArray);
                waveformCtx.fillStyle = 'rgb(253, 254, 254)'; 
                waveformCtx.fillRect(0, 0, waveformCanvas.width, waveformCanvas.height);
                const barWidth = (waveformCanvas.width / bufferLength) * 2.5;
                let barHeight; 
                let x = 0;
                for (let i = 0; i < bufferLength; i++) {
                    barHeight = dataArray[i] / 2;
                    waveformCtx.fillStyle = `rgb(${barHeight + 100}, 50, 50)`; 
                    waveformCtx.fillRect(x, waveformCanvas.height - barHeight, barWidth, barHeight);
                    x += barWidth + 1;
                }
                animationFrameId = requestAnimationFrame(drawWaveform);
            }

            function stopWaveformVisualization() {
                if (animationFrameId) cancelAnimationFrame(animationFrameId);
                if (audioContext && audioContext.state !== 'closed') audioContext.close();
            }

            // --- TTS (Deepgram Aura) + OUTPUT DEVICE SELECTION ---
            function browserSupportsOutputSelection() {
                return typeof HTMLMediaElement !== 'undefined' && 'sinkId' in HTMLMediaElement.prototype;
            }

            function setupTTS() {
                if (!ttsPlayBtn || !ttsStopBtn || !ttsVoiceSelect) return;

                ttsAudioElement = document.createElement('audio');
                ttsAudioElement.id = 'ttsAudioElement';
                ttsAudioElement.style.display = 'none';
                ttsAudioElement.preload = 'auto';
                document.body.appendChild(ttsAudioElement);

                ttsAudioElement.addEventListener('ended', () => {
                    updateStatus('TTS playback finished.', 'info');
                });

                ttsAudioElement.addEventListener('error', () => {
                    updateStatus('Error during TTS audio playback.', 'error');
                });

                ttsPlayBtn.addEventListener('click', handleTTSPlay);
                ttsStopBtn.addEventListener('click', handleTTSStop);

                if (audioOutputSelect && audioOutputSupportNote) {
                    audioOutputSelect.addEventListener('change', () => {
                        applySelectedOutputDevice();
                    });
                    initAudioOutputSelection();
                }
            }

            function initAudioOutputSelection() {
                if (!audioOutputSelect || !audioOutputSupportNote) return;

                audioOutputSelect.innerHTML = '<option value="">System default output</option>';

                if (!navigator.mediaDevices || !navigator.mediaDevices.enumerateDevices || !browserSupportsOutputSelection()) {
                    audioOutputSelect.disabled = true;
                    audioOutputSupportNote.textContent = 'Per-tab speaker selection is not supported in this browser. TTS will use the system default output.';
                    return;
                }

                audioOutputSupportNote.textContent = 'Loading available audio output devices...';

                navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {
                    stream.getTracks().forEach(track => track.stop());
                    return navigator.mediaDevices.enumerateDevices();
                }).then(devices => {
                    const audioOutputs = devices.filter(d => d.kind === 'audiooutput');
                    audioOutputs.forEach((device, index) => {
                        const option = document.createElement('option');
                        option.value = device.deviceId;
                        option.textContent = device.label || `Speaker ${index + 1}`;
                        audioOutputSelect.appendChild(option);
                    });
                    audioOutputSelect.disabled = false;
                    audioOutputSupportNote.textContent = 'Choose which speaker/headset should play TTS audio.';
                }).catch(err => {
                    console.warn('Unable to enumerate audio output devices:', err);
                    audioOutputSelect.disabled = true;
                    audioOutputSupportNote.textContent = 'Could not access audio outputs. TTS will use the system default output.';
                });
            }

            async function applySelectedOutputDevice() {
                if (!ttsAudioElement || !audioOutputSelect) return;
                if (!browserSupportsOutputSelection()) return;
                const deviceId = audioOutputSelect.value;
                try {
                    await ttsAudioElement.setSinkId(deviceId || '');
                    if (deviceId) {
                        updateStatus('TTS output set to selected device.', 'success');
                    } else {
                        updateStatus('TTS output reverted to system default.', 'info');
                    }
                } catch (err) {
                    console.error('Error applying output device', err);
                    updateStatus('Failed to switch TTS output device: ' + err.message, 'error');
                }
            }

            function getTranscriptTextForTTS() {
                let text = '';
                if (finalWebSpeechTranscript && finalWebSpeechTranscript.trim().length > 0) {
                    text = finalWebSpeechTranscript;
                } else if (transcriptOutputConvo) {
                    text = transcriptOutputConvo.innerText || transcriptOutputConvo.textContent || '';
                }
                text = text.trim();
                if (!text) return '';
                if (text.length > MAX_TTS_CHARS) {
                    text = text.slice(text.length - MAX_TTS_CHARS);
                }
                return text;
            }

            async function handleTTSPlay() {
                if (!DEEPGRAM_API_KEY) {
                    updateStatus('Deepgram API key missing. TTS is disabled.', 'error');
                    return;
                }
                if (!ttsAudioElement) {
                    updateStatus('TTS audio element is not initialized.', 'error');
                    return;
                }

                const text = getTranscriptTextForTTS();
                if (!text) {
                    updateStatus('No transcript available to read aloud yet.', 'error');
                    return;
                }

                if (ttsObjectUrl) {
                    URL.revokeObjectURL(ttsObjectUrl);
                    ttsObjectUrl = null;
                }

                const model = (ttsVoiceSelect && ttsVoiceSelect.value) ? ttsVoiceSelect.value : 'aura-2-thalia-en';

                showLoading('Generating TTS audio from transcript...');
                updateStatus('Requesting TTS audio...', 'info');

                try {
                    const url = 'https://api.deepgram.com/v1/speak?model=' + encodeURIComponent(model);
                    const response = await fetch(url, {
                        method: 'POST',
                        headers: {
                            'Authorization': 'Token ' + DEEPGRAM_API_KEY,
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({ text })
                    });

                    if (!response.ok) {
                        const errText = await response.text().catch(() => '');
                        throw new Error('TTS request failed (' + response.status + '): ' + errText);
                    }

                    const blob = await response.blob();
                    ttsObjectUrl = URL.createObjectURL(blob);
                    ttsAudioElement.src = ttsObjectUrl;

                    await applySelectedOutputDevice();
                    await ttsAudioElement.play();
                    updateStatus('Speaking transcript via TTS...', 'success');
                } catch (err) {
                    console.error('TTS error', err);
                    updateStatus('TTS error: ' + err.message, 'error');
                } finally {
                    hideLoading();
                }
            }

            function handleTTSStop() {
                if (ttsAudioElement && !ttsAudioElement.paused) {
                    ttsAudioElement.pause();
                    ttsAudioElement.currentTime = 0;
                    updateStatus('TTS playback stopped.', 'info');
                }
            }

            main();
        });
    </script>
</body>
</html>
